---
title: 'STAT 300: Written Assignment 2'
author: 'Aronn Grant Laurel (21232475)'
date: "April, 2025"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#if you do not have the package, type install.packages("name_of_the_package")
library(knitr)
library(zoo)
library(ggplot2)
```

### Question 1

(a) 

```{r, echo=TRUE}
student <- read.csv("students_data.csv", header = TRUE)
# head(student)

# Boxplot
ggplot(student, aes(x = TrainingMethod, y = ExamScore, fill = TrainingMethod)) +
  geom_boxplot() +
  labs(title = "Exam Score Distribution by Training Method",
       x = "Training Method",
       y = "Exam Score") +
  theme_minimal()

```
For our boxplot, In-Person Classes hold the highest median score and a relatively
wider inter-quartile range. Self-Study Method holdes the lowest median score while
Online Courses seem to have a smaller inter-quartile range. Overall,
students under the In-person class method seem to perform better on average.



```{r, echo=TRUE}
# Scatterplot
ggplot(student, aes(x = StudyHours, y = ExamScore, color = PreviousGPA)) +
  geom_point(alpha = 0.7) +
  scale_color_gradient(low = "blue", high = "red") +
  labs(title = "Study Hours vs. Exam Score (Colored by Previous GPA)",
       x = "Study Hours",
       y = "Exam Score",
       color = "Previous GPA") +
  theme_minimal()
```
Overall, we can observe a slight positive correlation between exam scores
and study hours. Furthermore, we can see that higher GPA holders (3.5 - 4.0) tend to
cluster at the upper right of the graph with high exam score and study hours. 
On the other hand, lower GPA holders (3.0 - 0) tend to be at the bottom right
of the scatterplot whose students have lower exam scores and study hours.
Therefore, we can say that students who have higher exam scores.


(b)

```{r, echo=TRUE}
student$TrainingMethod <- as.factor(student$TrainingMethod)
model <- lm(ExamScore ~ StudyHours + PreviousGPA + TrainingMethod, data = student)
summary(model)

```

$\ Exam Score = 46.3115 + 1.0688*(Study Hours) + 5.3071*(Previous GPA) - 2.8668*(Online Course) - 6.6728*(Self-Study) + epsilon$

For Coefficient: Study Hours
For an increase of 1 studying hour per week, we expect an increase in exam 
score by approximately 1.06 points

For Coefficient: PreviousGPA
For an increase of 1 Previous GPA point, we expect an increase in exam 
score by approximately 5.31 points

For Coefficient: TrainingMethod - Online Course
For students taking Online Course, they average 2.87 points lower on the exam
compared to those who attended In-person classes

For Coefficient: TrainingMethod - Self Study
For students who Self Studied, they average 6.67 points lower on the exam
compared to those who attended In-person classes

(c)
Given:
Study Hours per week = 8
Previous GPA = 3.8
Online Course = 1
Self Study = 0

```{r, echo=TRUE}
46.3115 + 1.0688*(8) + 5.3071*(3.8) - 2.8668*(1) - 6.6728*(0)
```
Therefore, this student's expected exam score is ~ 72.16

(d)
Null Hypothesis
The mean exam scores are equal across all methods
$\  H_0 : \mu_{Self Study} = \mu_{Online class} = \mu_{In Person}$

Alternative Hypothesis
At least one training method has a different mean exam score

```{r, echo=TRUE}
student$TrainingMethod <- as.factor(student$TrainingMethod)
anova_model <- aov(ExamScore ~ TrainingMethod, data = student)
summary(anova_model)

```
As we observe a small p-value 1.16e-10 for our categorical variable - Training
Method, we can reject the null hypothesis at 0.05 significance level. Therefore,
training methods is significant on exam scores.

(e)
```{r, echo=TRUE, fig.height = 7}
par(mfrow = c(2, 2))
plot(model)

```
Residual vs Fitted Plot: we can see no patterns which suggests Linearity  
and non-funneling and constant variance which suggests Homoscedasticity.

Normal Q-Q plot: We can see that our points roughly follows the 'normal'
diagonal line but with heavier tails and skewed, 
thus suggesting that the residuals may not be normally distributed (CLT Theorem).


(f)
```{r, echo=TRUE}
int_model <- lm(ExamScore ~ StudyHours * PreviousGPA + TrainingMethod, data = student)
summary(int_model)

summary(model)

AIC(model, int_model)

```
Looking at the p-values for the interaction term, we can see that it is a
higher pvalue at 0.3718 which is statistically insignificant and that
adding the interaction term may not be necessary. 

Furthermore, we can see that the Adjusted R-Squared between the two models
are very similar with the interaction model being higher by 0.0003.

We can also examine their AIC values to see the better model, and we can observe a 
lower AIC value for our linear model without interaction, thus suggesting that the 
interaction model may be a more insignificant explanatory variable. 

### Question 2
(a) 
```{r, echo=TRUE}
data <- read.csv("data.csv")

# Scatterplot Matrix
ggpairs(data,
        diag = list(continuous = "barDiag"),
        lower = list(continuous = "points"),
        upper = list(continuous = "cor"))

```
We can observe a strong correlation between Temperature and Concentration with 0.976
while the other relationship between other variables seems weaker with less than 0.1.
Looking at the scatter plots, we can observe a linear positive increasing pattern between
Temperature and Concentration while the other scatter plots seem more Non-linear without 
any obvious pattern.

(b)
```{r, echo=TRUE}
model_b <- lm(Concentration ~ Temp + pH, data = data)

summary(model_b)
```
The model equation is: $/ Concentration = 0.3701 + 3.6394(Temp) - 2.4722(pH) $

Coefficient Interpretations:
Temperature: For an increase in 1 Degree Celcius, we expect an increase in
pollutant concentration by 3.6394 units

pH: For an increase in 1 pH Level, we expect a decrease in pollutant concentration
by 2.4722 units

(c) A linear model seems appropriate because the scatter plot between Temperature
and Concentration displays a strong positive linear pattern. Our histograms
also do not have any obvious outliers that would suggest a more complex model.
Also looking into the p-value and Adjusted R-squared of our linear model,
I believe that it is adequate in capturing the relationship.

(d)
Since we have already fitted a multiple linear regression, we will extend
the model by applying an interaction term between Temperature and pH :

```{r, echo=TRUE}
model_d <- lm(Concentration ~ Temp * pH, data = data)

summary(model_d)
```
The model equation is: $/ Concentration = 2.93433 + 3.45312(Temp) - 2.82304(pH) + 0.02532 (Temp)(pH) $

Coefficient Interpretations:
Temperature: For an increase in 1 Degree Celcius, we expect an increase in
pollutant concentration by 3.45312 units

pH: For an increase in 1 pH Level, we expect a decrease in pollutant concentration
by 2.82304 units

Interaction Term: For an increase by 1 unit in both Temperature and pH Level, 
we expect an increase in pollutant concentration by 0.02532 units

Given the interaction's p-value to be quite high with 0.85422, that suggests that
the interaction term is not statistically significant at confidence level 5%. 
This suggests that including the interaction may not meaningfully improve the model.

(e)
(f)

### Question 3
(a)
(b)
(c)
(d)
(e)

### Question 4
(a)
(b)
(c)
(d)
(e)
(f)
